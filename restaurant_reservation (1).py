# -*- coding: utf-8 -*-
"""Restaurant Reservation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uYEv3_VMThsbT-6hyW16Us4QEuAHM0sY
"""

import pandas as pd
import numpy as np
import geopandas as gpd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv("/content/drive/MyDrive/food1.csv")

df.head()

df.columns

df.info()

"""Checking for duplicate values"""

df.describe()

top_restaurants = df.sort_values(by=['aggregate_rating'], ascending=False)[:20]
top_restaurants

"""**Plotting**"""

sns.pairplot(df)

sns.distplot(df['average_cost_for_one'])

sns.heatmap(df.corr())

fig, ax = plt.subplots(figsize=(12,10))
sns.barplot(x = 'aggregate_rating', y = 'name', data = top_restaurants, ax=ax);
plt.savefig('top20_restaurants.png')
plt.show()

df['aggregate_rating'].astype("float")

"""# **Count of ratings as between "1 and 2", "2 and 3", "3 and 4", and "4 and 5"**"""

df['aggregate_rating'].unique()

df['aggregate_rating'].min()

df['aggregate_rating'].max()

df['aggregate_rating']=df['aggregate_rating'].astype(float)

((df['aggregate_rating']>=1) & (df['aggregate_rating']<2)).sum()

((df['aggregate_rating']>=2) & (df['aggregate_rating']<3)).sum()

((df['aggregate_rating']>=3) & (df['aggregate_rating']<4)).sum()

(df['aggregate_rating']>=4).sum()

"""
# **Plotting the counts with the help of pie chart**"""

slices=[((df['aggregate_rating']>=1) & (df['aggregate_rating']<2)).sum(),
        ((df['aggregate_rating']>=2) & (df['aggregate_rating']<3)).sum(),
        ((df['aggregate_rating']>=3) & (df['aggregate_rating']<4)).sum(),
        (df['aggregate_rating']>=4).sum()
        ]

labels=['1<rate<2','2<rate<3','3<rate<4','>4']
colors = ['#ff3333','#c2c2d6','#6699ff']
plt.pie(slices,colors=colors, labels=labels, autopct='%1.0f%%', pctdistance=.5, labeldistance=1.2,shadow=True)
fig = plt.gcf()
plt.title("Percentage of Restaurants according to their ratings")

fig.set_size_inches(10,10)
plt.show()

"""# **KNN**"""

from sklearn.neighbors import NearestNeighbors
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Preprocess your data (label encoding for categorical variables)
label_encoder = LabelEncoder()
df['cuisine_encoded'] = label_encoder.fit_transform(df['cuisines'])
df['locality_encoded'] = label_encoder.fit_transform(df['locality'])

# Scale numeric variables if needed (e.g., Min-Max scaling)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df['average_cost_for_one'] = scaler.fit_transform(df[['average_cost_for_one']])
#df['locality_encoded']
#df['cuisine_encoded']
#df['average_cost_for_one']

# Select relevant features for KNN
X = df[['cuisine_encoded', 'average_cost_for_one', 'locality_encoded']]

# Create a KNN model
knn = NearestNeighbors(n_neighbors=5)
knn.fit(X)

# Define the input data (all values should be numeric)
input_data = [[57,  0.418182, 12]]

# Find nearest neighbors (restaurants) to the given input
distances, indices = knn.kneighbors(input_data)

# Get recommended restaurants based on distances and indices
recommended_restaurants = df.iloc[indices[0]]

# Print the recommended restaurants
print(recommended_restaurants)

from sklearn.neighbors import NearestNeighbors
import pandas as pd

# Assuming you have already fit a KNN model to the data
# Define and fit the KNN model
knn = NearestNeighbors(n_neighbors=5)
X = df[['cuisine_encoded', 'average_cost_for_one', 'locality_encoded']]
knn.fit(X)
# Input data for recommendations
input_data = [[57, 0.418182, 12]]

# Find k-nearest neighbors
distances, indices = knn.kneighbors(input_data)

# Get the recommended restaurants based on distances and indices
recommended_restaurants = df.iloc[indices[0]]['name'].tolist()

# Assuming you have user interaction data with relevant items
# This is just a conceptual example; replace it with your actual data
relevant_items = ['The Drowning Street ', 'The Jail Cafe', 'Sheroes Hangout']

# Calculate precision and recall for the recommendations
def calculate_precision_recall(recommended_items, relevant_items):
    # Calculate the number of recommended items that are relevant
    true_positives = len(set(recommended_items).intersection(relevant_items))

    # Calculate precision
    precision = true_positives / len(recommended_items) if len(recommended_items) > 0 else 0.0

    # Calculate recall
    recall = true_positives / len(relevant_items) if len(relevant_items) > 0 else 0.0

    return precision, recall

precision, recall = calculate_precision_recall(recommended_restaurants, relevant_items)

print(f"Precision: {precision}")
print(f"Recall: {recall}")

"""# **Linear Regression**"""

from sklearn.linear_model import LinearRegression

X = df[['cuisine_encoded', 'average_cost_for_one', 'locality_encoded']]

y = df['aggregate_rating']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

regression_model = LinearRegression()
regression_model.fit(X_train, y_train)

print(regression_model.intercept_)

coeff_df = pd.DataFrame(regression_model.coef_,X.columns,columns = ["coefficient"])
coeff_df

y_pred = regression_model.predict(X_test)

plt.scatter(y_test,y_pred)

from sklearn import metrics

# Evaluate the model
print("Mean Squared Error:",metrics.mean_squared_error(y_test, y_pred))
print('MAE :',metrics.mean_absolute_error(y_test,y_pred))
print('RMSE :',np.sqrt(mean_squared_error(y_test,y_pred)))

"""# **KMeans Clustering**"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Assuming you have features that you want to use for clustering
features = ['cuisine_encoded', 'average_cost_for_one', 'locality_encoded']
X = df[features]
distortions = []

# Standardize your data (important for k-means)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Specify the number of clusters (k)
k = 5  # You can change this to the desired number of clusters

# Create a KMeans model and fit it to your data
kmeans = KMeans(n_clusters=k, random_state=0)
kmeans.fit(X_scaled)

# Add cluster labels to your original DataFrame
df['cluster'] = kmeans.labels_

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Assuming data is stored in X
k_values = range(1, 11)  # Test different values of k
distortions = []

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(X)
    distortions.append(kmeans.inertia_)

# Plotting the Elbow Method
plt.figure(figsize=(12, 8))
plt.plot(k_values, distortions, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Distortion')
plt.title('Elbow Method For Optimal k')
plt.show()

import pickle

# Pickle the K-Means model
with open('knn_model.pkl', 'wb') as model_file:
    pickle.dump(knn, model_file)

# Load the K-Means model
with open('knn_model.pkl', 'rb') as model_file:
    loaded_knn = pickle.load(model_file)

"""- **Precision** is a measure of the accuracy of the recommendations. It tells you what proportion of the recommended items were relevant to the user. In your case, a precision of 0.4 means that 40% of the recommended restaurants were relevant to the user.

- **Recall** is a measure of the coverage of the relevant items in the recommendations. It tells you what proportion of the relevant items were successfully recommended. A recall of approximately 0.67 means that 67% of the relevant restaurants were included in the recommendations.

These values are typically between 0 and 1, with higher values indicating better performance. So, in this case, a precision of 0.4 and a recall of 0.67 suggest that the recommendations are somewhat accurate and manage to cover a significant portion of the relevant items, but there is room for improvement.

**we wont get accuracy bcz we r not performin classification or regression, you're just finding neighbors in your feature space based on distances.**

The NearestNeighbors class in scikit-learn is primarily used for unsupervised tasks like finding nearest neighbors, clustering, and dimensionality reduction. It doesn't provide a straightforward way to compute confusion matrices or classification reports because these are typically used in supervised machine learning tasks like classification or regression, where you have both features (X) and labels (y).

The metrics you've mentioned are common evaluation metrics for regression models, like the linear regression model you applied to your restaurant recommendation problem. Here's what they mean:

1. **Mean Squared Error (MSE)**: MSE measures the average of the squared differences between the actual (observed) values and the predicted values. In your case, the MSE of approximately 0.107 suggests that, on average, your model's predictions are quite close to the actual user ratings. Lower MSE values indicate better model performance, with zero being a perfect fit.

2. **Mean Absolute Error (MAE)**: MAE is another measure of the absolute errors between the actual and predicted values. MAE measures the average of these absolute differences. In your case, the MAE of approximately 0.235 indicates the average magnitude of prediction errors. Smaller MAE values are preferable, as they mean the model's predictions are closer to the actual values.

3. **Root Mean Squared Error (RMSE)**: RMSE is the square root of the MSE and is used to measure the standard deviation of the prediction errors. An RMSE value of approximately 0.328 suggests that, on average, your model's predictions are typically off by about 0.328 units concerning user ratings. Like the MSE, lower RMSE values are better.

In summary, the lower these error metrics (MSE, MAE, RMSE), the better your model's performance in predicting user ratings. Your model seems to perform reasonably well, with the given metrics indicating that it's making predictions that are relatively close to the actual user ratings.

The "Elbow Method" is a technique to determine the optimal number of clusters for a K-Means clustering algorithm. It looks for an "elbow" point in the plot where the distortion starts to decrease at a slower rate. The number of clusters corresponding to this point is considered optimal for clustering your data. The code helps you visualize this concept by plotting distortions for different values of k.
"""

